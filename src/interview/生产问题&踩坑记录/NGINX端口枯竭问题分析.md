# NGINX端口枯竭问题分析
<!--page header-->

今天x系统凌晨放量，某接口tps达到限流阈值，持续一段时间后，tps骤降至0，排查后，定位问题为nginx端口枯竭，ng与x系统的探活连续失败，x系统配置的vip被摘导致。
<a name="aced919d"></a>
## NGINX端口枯竭

nginx在服务期间，需要与其他服务建立tcp连接，而建立连接需要端口，而服务器的端口数仅有65535，而linux下，可用于建立客户端tcp连接的端口仅有28282个，因此nginx在面临请求量突增的情况下，将有可能因为服务器上的可用端口数耗尽，而导致nginx服务不可用，即描述的端口枯竭。分析解决该问题，涉及到以下知识。

<a name="953ec37c"></a>
## TCP建立连接

- 服务器端先启动，创建一个监听socket，并把socket绑定到一个固定的开放端口，然后监听这个socket以等待客户端的连接请求发送过来；
- 客户端后启动，创建一个socket，并向服务器申请一闲置端口，然后用它连接到服务端的监听socket；
- 服务端在监听socket等着客户端连接进来之后，在服务端为这个连接产生一个业务socket，后续服务端就用此业务socket给这个客户端通信；
- 客户端在连接建立后，就可以向服务端发送请求数据；
- 客户端在请求满足之后，主动关闭连接

这里仅为分析端口枯竭问题，就不提及握手问题了。

<a name="23178283"></a>
## TCP断开连接

- 客户端发送FIN信号，用来通知关闭与服务端的数据传送，客户端进入等待关闭状态。
- 服务端收到FIN信号后，发送一个ACK信息给服务端，服务端进入关闭等待状态。
- 服务端这时候再发送一个FIN信号给客户端，用来通知关闭与客户端的数据传送，服务端进入最后关闭状态。
- 客户端收到FIN信号后，发送一个ACK信号给服务端，服务端收到信号后关闭该连接，与此同时客户端进入TIME_WAIT状态(该状态将持续2MSL时间)，随后再释放出此前申请的端口。

MSL:传输数据段最大生存时间

<a name="67b87d4f"></a>
## TIME_WAIT状态

上述TCP断开连接时提及到TIME_WAIT状态，那它又是什么？

在描述这个状态前，我们需知道，网络是错综复杂的，一段段的数据流在传输过程中是无序的。如果我们在TCP断开连接时不处理好，将有可能出现某端口在结束上一段连接不久，即建立一个新的连接，而这时候属于上一段连接的数据流延迟到了，导致会话错乱。我们要保证延迟数据不影响会话，就需要延迟数据不可达，那么就要它网络上自己消失，而恰好我们的延迟数据它只能在网络上存活一个MSL时间。同时为保证TCP连接的远程被正确关闭，即等待被动关闭连接的一方收到FIN对应的ACK消息，因此就有了TIME_WAIT这个状态，那么问题又来了，延迟数据明明只活一个MSL时间，那么为什么TIME_WAIT是2MSL，可考虑边缘情况，客户端发送ack后，这个ack也在网络上呆了接近一个MSL，这时候服务端因为还没收到ack，再重发了一个FIN给客户端，而这个FIN信号这时候已经是无用的延迟数据段了，客户端不应该接受到了，那么只能等这个FIN也在网络上消失掉，这样就可以理解为什么定义time_wait为2msl了。

<a name="2fc180ee"></a>
## TIME_WAIT对Nginx影响

按rpc793文档规定，一个MSL为60秒，而早期的linux也遵守这规定。尽管后期优化，不同的服务器在MSL的制定大概范围都是30s到60s。对于一个Nginx服务器，如果在请求高峰期。Nginx在120s内无法向服务器获取到可用端口，那么Nginx服务将崩溃。

<a name="2f15edad"></a>
## 优化/解决方案

优化思路为，TIME_WAIT状态时，使端口重用：

1、修改/etc/sysctl.conf文件

```
net.ipv4.tcp_tw_reuse=1 #让TIME_WAIT状态可以重用，这样即使TIME_WAIT占满了所有端口，也不会拒绝新的请求造成障碍 默认是0
```

2、Nginx设置keepalive长链接，设置端口可重用最大空闲连接数等信息

```
upstream demo {
    server 127.0.0.1:8080;
    keepalive 100;
}
```

以上仅为优化方案，要解决这个问题，还是需要从nginx集群架构上解决。


<!--page footer-->
- 原文: <https://www.yuque.com/dabin-1eu6s/plc2v4/gpvqvqhnmsx5fbr9>